{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "from WikiCatUtils import (\n",
    "    Cache,\n",
    "    read_categories,\n",
    "    get_fullpath,\n",
    "    resample_to_equal)\n",
    "from os.path import exists\n",
    "from os import mkdir\n",
    "import numpy as np\n",
    "\n",
    "if not exists('/tmp/WikiCat'):                                                               \n",
    "    mkdir('/tmp/WikiCat')                                                                    \n",
    "categories_to_download = read_categories(get_fullpath('example_cats.txt'))                 \n",
    "cache = Cache(get_fullpath('/tmp/WikiCat/cache'), verbosity=0)                            \n",
    "for category_uri in categories_to_download:                                                  \n",
    "    cache.loadCategory(category_uri, only_use_cached=True)# maxlinks=100)\n",
    "\n",
    "\n",
    "dset, label_map = cache.get_dataset(0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 634, 'test': 215, 'val': 213}\n",
      "1062\n",
      "\t0\t1\t2\t3\t4\t5\t6\n",
      "test\t0.03\t0.03\t0.06\t0.05\t0.10\t0.10\t0.63\n",
      "train\t0.03\t0.03\t0.06\t0.05\t0.10\t0.10\t0.64\n",
      "val\t0.03\t0.03\t0.06\t0.05\t0.10\t0.10\t0.63\n",
      "{0: 'Category:Organs (anatomy)', 1: 'Category:Cancer', 2: 'Category:Medical devices', 3: 'Category:Machine learning algorithms', 4: 'Category:Infectious diseases', 5: 'Category:Congenital disorders', 6: 'Category:Rare diseases'}\n"
     ]
    }
   ],
   "source": [
    "# dataset statistics\n",
    "print({split:len(dset[split][0]) for split in dset})\n",
    "reverse_label_map = {label_map[category]:category for category in label_map}\n",
    "indices = reverse_label_map.keys()\n",
    "print(len(cache.contents))\n",
    "print(\"\\t%s\" % ('\\t'.join(map(str,indices))))\n",
    "for split in dset:\n",
    "    category_percentages = []\n",
    "    for category_ind in indices:\n",
    "        category_percentages.append(dset[split][1].count(category_ind)\n",
    "                                    / len(dset[split][1]))\n",
    "    print(\"%s\\t%s\" % (split, '\\t'.join(map(lambda x: \"%0.2f\" % x, category_percentages))))\n",
    "\n",
    "print(reverse_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(634, 11091)\n"
     ]
    }
   ],
   "source": [
    "# this is extremely unbalanced, but we'll see how well it does despite that\n",
    "# let's try BoW first as a baseline\n",
    "\n",
    "import re\n",
    "from string import punctuation\n",
    "punct_matcher = re.compile(r'[{}]+'.format(re.escape(punctuation)))\n",
    "space_matcher = re.compile(r'\\s+')\n",
    "def tokenize(article):\n",
    "    # instead of tokenizing by anything fancy we'll just separate by spaces\n",
    "    # and remove punctuation / quotes / hyphens\n",
    "    stripped = article.rstrip().lstrip()\n",
    "    no_punct = re.sub(punct_matcher, '', stripped)\n",
    "    tokenized = re.split(space_matcher, no_punct)\n",
    "    return tokenized\n",
    "\n",
    "def make_vocab(articles):\n",
    "    vocab = {}\n",
    "    for article in articles:\n",
    "        tokenized_article = tokenize(article)\n",
    "        for word in tokenized_article:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "            else:\n",
    "                vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "def convert_classes(labels):\n",
    "    # most ml packages expect multiclass labels to be one-hot encoded\n",
    "    n_classes = max(labels) + 1\n",
    "    label_indices = list(zip(*[(ind, class_ind) for ind, class_ind in enumerate(labels)]))\n",
    "    label_mat = np.zeros((len(labels), n_classes))\n",
    "    label_mat[label_indices] = 1\n",
    "    return label_mat\n",
    "\n",
    "\n",
    "#train_vocab = make_vocab(dset['train'][0])\n",
    "#print(len(train_vocab))\n",
    "class BoW_transformer:\n",
    "    def __init__(self, minsample=5, tfidf_weights=False):\n",
    "        self.use_tfidf = tfidf_weights    \n",
    "        self.minsample = minsample\n",
    "        \n",
    "    def get_idf(self, articles):\n",
    "        # for every word in the vocab, get its inverse doc freq\n",
    "        self.idf = np.zeros((1,self.vocab_size))\n",
    "        vocab_appears_in = {word:np.zeros(len(articles), dtype=np.int) \n",
    "                            for word in self.lookup}\n",
    "        for ind, article in enumerate(articles):\n",
    "            for word in tokenize(article):\n",
    "                if word in self.lookup:\n",
    "                    vocab_appears_in[word][ind] = 1\n",
    "        for word in self.lookup:\n",
    "            self.idf[:,self.lookup[word]] = np.log(1 + len(articles) / np.sum(\n",
    "                                                vocab_appears_in[word]))\n",
    "            \n",
    "    def fit(self, articles):\n",
    "        # figure out the vocab and tfidf stuff\n",
    "        self.vocab = make_vocab(articles)\n",
    "        vocab_sorted = sorted(self.vocab.keys())\n",
    "        vocab_minsampled = list(filter(lambda x: self.vocab[x] >= self.minsample, \n",
    "                                       vocab_sorted))\n",
    "        self.lookup = {word:ind for ind, word in enumerate(vocab_minsampled)}\n",
    "        self.vocab_size = len(self.lookup)\n",
    "        if self.use_tfidf:\n",
    "            self.get_idf(articles)\n",
    "    \n",
    "    def transform_single(self, tokenized_article):\n",
    "        bowvec = np.zeros((1,self.vocab_size), dtype=np.float32)\n",
    "        for word in tokenized_article:\n",
    "            if word in self.lookup:\n",
    "                word_ind = self.lookup[word]\n",
    "                bowvec[:,word_ind] = 1\n",
    "        if self.use_tfidf:\n",
    "            # implement the log-scaled frequency\n",
    "            bowvec = np.log(1 + bowvec)\n",
    "            bowvec *= self.idf\n",
    "        return bowvec\n",
    "    \n",
    "    def transform(self, articles):\n",
    "        return np.vstack([self.transform_single(tokenize(article)) \n",
    "                          for article in articles])\n",
    "                \n",
    "bow_maker = BoW_transformer()\n",
    "bow_maker.fit(dset['train'][0])\n",
    "train_bow = bow_maker.transform(dset['train'][0])\n",
    "train_labels = convert_classes(dset['train'][1])\n",
    "print(train_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(tokenize(dset['train'][0][0]))\n",
    "#print(dset['train'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_linear = LogisticRegression()\n",
    "bow_linear.fit(train_bow, dset['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[ 18  20  36  31  61  63 405]\n"
     ]
    }
   ],
   "source": [
    "print(bow_linear.score(train_bow, dset['train'][1]))\n",
    "# This result (~100% accuracy) shows the inherent issue with having a ton of features\n",
    "# and very little data -- obviously we're well beyond LogisticRegression's VC dimension\n",
    "print(np.histogram(bow_linear.predict(train_bow), bins=train_labels.shape[1])[0])\n",
    "# this is real bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4   3  10  11  16   6 163]\n",
      "[  6   7  12  11  21  21 135]\n",
      "0.821596244131\n"
     ]
    }
   ],
   "source": [
    "# We can very easily see the overfitting here\n",
    "val_bow = bow_maker.transform(dset['val'][0])\n",
    "val_labels = convert_classes(dset['val'][1])\n",
    "\n",
    "print(np.histogram(bow_linear.predict(val_bow), bins=val_labels.shape[1])[0])\n",
    "print(np.histogram(np.argmax(val_labels, axis=1), bins=val_labels.shape[1])[0])\n",
    "print(bow_linear.score(val_bow, dset['val'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n",
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839497516657\n",
      "0.0380556018582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    }
   ],
   "source": [
    "# Despite the overfitting it's worth noting that it performs rather well.\n",
    "# Let's see how this holds up under cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cv_scores = cross_val_score(LogisticRegression(), train_bow, dset['train'][1], cv=5)\n",
    "print(np.average(cv_scores))\n",
    "print(np.std(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So it seems to be getting ~84% accuracy pretty regularly. We'll see if we can improve\n",
    "# it using TF-IDF weights -- however we need to be careful since we need to learn these \n",
    "# weights only on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/base.py:175: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  args, varargs, kw, default = inspect.getargspec(init)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tfidf_maker = BoW_transformer(tfidf_weights=True)\n",
    "bow_tfidf_maker.fit(dset['train'][0])\n",
    "train_bow_tfidf = bow_tfidf_maker.transform(dset['train'][0])\n",
    "\n",
    "\n",
    "bow_linear_tfidf = LogisticRegression()\n",
    "bow_linear_tfidf.fit(train_bow_tfidf, dset['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_linear_tfidf.score(train_bow_tfidf, dset['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82629107981220662"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_bow_tfidf = bow_tfidf_maker.transform(dset['val'][0])\n",
    "bow_linear_tfidf.score(val_bow_tfidf, dset['val'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just to be sure we'll make a simple pipeliner that can take the representer and learner\n",
    "# in one so we can try out "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
